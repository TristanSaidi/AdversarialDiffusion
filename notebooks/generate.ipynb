{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm import tqdm\n",
    "os.chdir('../src')\n",
    "\n",
    "from models.diffusion import Diffusion, CondDiffusion\n",
    "from models.unet import Unet\n",
    "from models.condunet import condUnet\n",
    "\n",
    "reverse_transform = transforms.Compose([transforms.Lambda(lambda x: (x+1)/2), transforms.ToPILImage()])\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../outputs/models/mnist_diffusion_epoch_19.pt'\n",
    "\n",
    "# model params\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "data_shape = (28, 28)\n",
    "channels = 1\n",
    "dim_mults = (1, 2, 4, )\n",
    "T = 300\n",
    "\n",
    "unet = Unet(\n",
    "    dim=data_shape[0],\n",
    "    channels=channels,\n",
    "    dim_mults=dim_mults,\n",
    ")\n",
    "unet.load_state_dict(torch.load(model_path))\n",
    "\n",
    "diffusion = Diffusion(\n",
    "    model=unet,\n",
    "    data_shape=data_shape,\n",
    "    T=T,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples\n",
    "sample = diffusion.sample().squeeze(0)\n",
    "sample = reverse_transform(sample)\n",
    "plt.imshow(sample, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../outputs/models/cond_mnist_diffusion_epoch_19.pt'\n",
    "\n",
    "# model params\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "data_shape = (28, 28)\n",
    "channels = 1\n",
    "dim_mults = (1, 2, 4, )\n",
    "T = 300\n",
    "\n",
    "net = condUnet(\n",
    "    dim=data_shape[0],\n",
    "    channels=channels,\n",
    "    dim_mults=dim_mults,\n",
    "    num_classes=10,\n",
    ")\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "mnist_cond_diffusion = CondDiffusion(\n",
    "    model=net,\n",
    "    data_shape=data_shape,\n",
    "    T=T,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples\n",
    "for i in range(10):\n",
    "    label = torch.tensor([i]).to(device)\n",
    "    sample = mnist_cond_diffusion.sample(label).squeeze(0)\n",
    "    sample = reverse_transform(sample)\n",
    "    plt.imshow(sample, cmap='gray')\n",
    "    plt.savefig(f'../outputs/images/mnist_cond_diffusion_generated_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../outputs/models/cond_fashion_mnist_diffusion_linear_sched_epoch_19.pt'\n",
    "\n",
    "# model params\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "data_shape = (28, 28)\n",
    "channels = 1\n",
    "dim_mults = (1, 2, 4, )\n",
    "T = 300\n",
    "\n",
    "net = condUnet(\n",
    "    dim=data_shape[0],\n",
    "    channels=channels,\n",
    "    dim_mults=dim_mults,\n",
    "    num_classes=10,\n",
    ")\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "fashion_mnist_cond_diffusion = CondDiffusion(\n",
    "    model=net,\n",
    "    data_shape=data_shape,\n",
    "    T=T,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples\n",
    "for i in range(10):\n",
    "    label = torch.tensor([i]).to(device)\n",
    "    sample = fashion_mnist_cond_diffusion.sample(label).squeeze(0)\n",
    "    sample = reverse_transform(sample)\n",
    "    plt.imshow(sample, cmap='gray')\n",
    "    plt.savefig(f'../outputs/images/fashion_mnist_cond_diffusion_generated_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleaved_label_sample(noise, labels, ts, model):\n",
    "    assert len(labels) == len(ts)\n",
    "    x_0 = noise\n",
    "    x_t = x_0\n",
    "    for label, t in zip(labels, ts):\n",
    "        t = torch.tensor([t]).to(model.device)\n",
    "        x_t = model.sample_p_t(x_t, t, label)\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit_sample(self, y):\n",
    "    \" sample from the model and return latent noise\"\n",
    "    y = torch.tensor([y]).to(self.device)\n",
    "    x_0 = torch.randn(size=self.data_shape).to(self.device)\n",
    "    x_0 = x_0[None, None, :, :] # (B, C, H, W)\n",
    "    x_t = x_0\n",
    "    latents = [x_0]\n",
    "    for t in reversed(range(self.T)):\n",
    "        t = torch.tensor([t]).to(self.device)\n",
    "        x_t = self.sample_p_t(x_t, t, y)\n",
    "        latents.append(x_t)\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = explicit_sample(fashion_mnist_cond_diffusion, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from the notion page under header \n",
    "\n",
    "### Exploring Class-Conditional Denoising ###\n",
    "\n",
    "label_a = 0\n",
    "label_b = 1\n",
    "\n",
    "# noises = fashion_mnist_cond_diffusion.explicit_sample(label_a)\n",
    "samples = []\n",
    "ts = list(reversed(range(fashion_mnist_cond_diffusion.T)))\n",
    "for i in tqdm(range(0, fashion_mnist_cond_diffusion.T, 30)):\n",
    "    noise = noises[i]\n",
    "    remaining_denoising_steps = fashion_mnist_cond_diffusion.T-i\n",
    "    labels = torch.tensor([label_b]*remaining_denoising_steps).to(fashion_mnist_cond_diffusion.device)\n",
    "    ts_cur = ts[-remaining_denoising_steps:]\n",
    "    sample = interleaved_label_sample(noise, labels, ts_cur, fashion_mnist_cond_diffusion)\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(noises):\n",
    "    if i % 30 == 0:\n",
    "        sample = reverse_transform(sample.squeeze(0))\n",
    "        plt.figure()\n",
    "        plt.imshow(sample, cmap='gray')\n",
    "        plt.savefig(f'../outputs/images/fashion_mnist_cond_diffusion_noise_{i}_class_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
